#!/usr/bin/env python3
"""
Self-Annealing Token Optimizer
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Filename: execution/system/self_annealing_optimizer.py
Purpose: Automatically detect inefficient token usage patterns
         and update directives with learnings
Author: 97LAYER System
Date: 2026-02-15
"""

import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SelfAnnealingOptimizer:
    """
    ìê°€ ê°œì„ í˜• í† í° ìµœì í™” ì‹œìŠ¤í…œ
    - ë¹„íš¨ìœ¨ì  íŒ¨í„´ ìë™ ê°ì§€
    - Directive ìë™ ì—…ë°ì´íŠ¸
    - ìµœì í™” ì „ëµ í•™ìŠµ
    """

    def __init__(self, project_root: str = None):
        self.root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.cache_dir = self.root / ".tmp" / "token_cache"
        self.learning_file = self.root / ".tmp" / "optimization_learnings.json"
        self.learning_file.parent.mkdir(parents=True, exist_ok=True)
        self.directive_file = self.root / "directives" / "token_optimization_protocol.md"

    def analyze_inefficiencies(self) -> Dict[str, List[Dict]]:
        """ë¹„íš¨ìœ¨ì ì¸ íŒ¨í„´ ë¶„ì„"""
        logger.info("Analyzing token usage patterns for inefficiencies...")

        inefficiencies = {
            'large_prompts': [],
            'repeated_queries': [],
            'uncached_queries': [],
            'missed_opportunities': []
        }

        if not self.cache_dir.exists():
            logger.warning("No cache directory found")
            return inefficiencies

        cache_files = list(self.cache_dir.glob("*.json"))
        if not cache_files:
            logger.warning("No cache files found")
            return inefficiencies

        prompt_hashes = defaultdict(int)
        large_prompts = []

        for cache_file in cache_files:
            if cache_file.name == "optimization_stats.json":
                continue

            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached = json.load(f)

                prompt_hash = cached.get('prompt_hash')
                response = cached.get('response', '')
                metadata = cached.get('metadata', {})

                # ë°˜ë³µ ì¿¼ë¦¬ ê°ì§€
                prompt_hashes[prompt_hash] += 1

                # í° í”„ë¡¬í”„íŠ¸ ê°ì§€ (ì¶”ì • í† í° > 2000)
                estimated_tokens = len(response) // 4
                if estimated_tokens > 2000:
                    large_prompts.append({
                        'hash': prompt_hash,
                        'estimated_tokens': estimated_tokens,
                        'context': metadata.get('context', 'unknown'),
                        'timestamp': cached.get('timestamp')
                    })

            except Exception as e:
                logger.error(f"Error analyzing cache file {cache_file}: {e}")

        # ë°˜ë³µ ì¿¼ë¦¬ (2íšŒ ì´ìƒ)
        for prompt_hash, count in prompt_hashes.items():
            if count > 1:
                inefficiencies['repeated_queries'].append({
                    'hash': prompt_hash,
                    'count': count,
                    'recommendation': 'This query was repeated. Good that it was cached!'
                })

        # í° í”„ë¡¬í”„íŠ¸ (ìƒìœ„ 10ê°œ)
        inefficiencies['large_prompts'] = sorted(
            large_prompts,
            key=lambda x: x['estimated_tokens'],
            reverse=True
        )[:10]

        # ë†“ì¹œ ê¸°íšŒ ê³„ì‚°
        total_queries = len(cache_files) - 1  # optimization_stats.json ì œì™¸
        repeated_count = sum(1 for c in prompt_hashes.values() if c > 1)
        cache_potential = (repeated_count / max(total_queries, 1)) * 100

        if cache_potential < 30:
            inefficiencies['missed_opportunities'].append({
                'type': 'low_cache_reuse',
                'metric': f'{cache_potential:.1f}%',
                'recommendation': 'Consider increasing cache duration or identifying more reusable queries'
            })

        logger.info(f"âœ“ Found {len(inefficiencies['large_prompts'])} large prompts, "
                   f"{len(inefficiencies['repeated_queries'])} repeated queries")

        return inefficiencies

    def generate_learnings(self, inefficiencies: Dict) -> List[Dict]:
        """ë¹„íš¨ìœ¨ì„±ì„ í•™ìŠµ í•­ëª©ìœ¼ë¡œ ë³€í™˜"""
        learnings = []

        # í° í”„ë¡¬í”„íŠ¸ í•™ìŠµ
        large_prompts = inefficiencies.get('large_prompts', [])
        if large_prompts:
            avg_tokens = sum(p['estimated_tokens'] for p in large_prompts) / len(large_prompts)

            if avg_tokens > 3000:
                learnings.append({
                    'category': 'prompt_size',
                    'severity': 'high',
                    'observation': f'Average large prompt size: {avg_tokens:.0f} tokens',
                    'learning': 'Prompts are consistently large. Implement snippet extraction before querying.',
                    'action': 'Update code to use token_optimizer.extract_relevant_snippets()',
                    'expected_savings': '60-80% token reduction',
                    'timestamp': datetime.now().isoformat()
                })

        # ë°˜ë³µ ì¿¼ë¦¬ í•™ìŠµ
        repeated = inefficiencies.get('repeated_queries', [])
        if len(repeated) > 5:
            learnings.append({
                'category': 'query_patterns',
                'severity': 'medium',
                'observation': f'{len(repeated)} queries were repeated',
                'learning': 'Good caching behavior detected. Cache hit rate is improving.',
                'action': 'Continue current caching strategy',
                'expected_savings': 'Maintained',
                'timestamp': datetime.now().isoformat()
            })

        # ë†“ì¹œ ê¸°íšŒ í•™ìŠµ
        missed = inefficiencies.get('missed_opportunities', [])
        for opportunity in missed:
            learnings.append({
                'category': 'missed_optimization',
                'severity': 'high',
                'observation': opportunity['recommendation'],
                'learning': 'Cache reuse is low. Query patterns may be too diverse.',
                'action': 'Review token_optimization_protocol.md section on cache strategies',
                'expected_savings': '20-40% additional reduction',
                'timestamp': datetime.now().isoformat()
            })

        return learnings

    def save_learnings(self, learnings: List[Dict]):
        """í•™ìŠµ í•­ëª© ì €ì¥"""
        existing_learnings = []

        if self.learning_file.exists():
            try:
                with open(self.learning_file, 'r', encoding='utf-8') as f:
                    existing_learnings = json.load(f)
            except Exception as e:
                logger.error(f"Error loading existing learnings: {e}")

        # ìƒˆ í•™ìŠµ ì¶”ê°€
        all_learnings = existing_learnings + learnings

        # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
        all_learnings = all_learnings[-50:]

        try:
            with open(self.learning_file, 'w', encoding='utf-8') as f:
                json.dump(all_learnings, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ“ Saved {len(learnings)} new learnings")
        except Exception as e:
            logger.error(f"Error saving learnings: {e}")

    def update_directive_if_needed(self, learnings: List[Dict]) -> bool:
        """í•„ìš” ì‹œ directive ìë™ ì—…ë°ì´íŠ¸"""
        if not learnings:
            logger.info("No significant learnings to update directive")
            return False

        # High severity í•™ìŠµë§Œ directiveì— ë°˜ì˜
        high_severity = [l for l in learnings if l.get('severity') == 'high']

        if not high_severity:
            logger.info("No high-severity learnings found")
            return False

        if not self.directive_file.exists():
            logger.warning(f"Directive file not found: {self.directive_file}")
            return False

        try:
            with open(self.directive_file, 'r', encoding='utf-8') as f:
                directive_content = f.read()

            # Learning section ì°¾ê¸° ë˜ëŠ” ìƒì„±
            learning_section = "\n\n---\n\n## ğŸ”„ Recent Learnings (Auto-Generated)\n\n"
            learning_section += f"_Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}_\n\n"

            for learning in high_severity[-3:]:  # ìµœê·¼ 3ê°œë§Œ
                learning_section += f"### {learning['category'].replace('_', ' ').title()}\n\n"
                learning_section += f"**Observation**: {learning['observation']}\n\n"
                learning_section += f"**Learning**: {learning['learning']}\n\n"
                learning_section += f"**Action**: {learning['action']}\n\n"
                learning_section += f"**Expected Savings**: {learning['expected_savings']}\n\n"

            # ê¸°ì¡´ learning section ì œê±°
            if "## ğŸ”„ Recent Learnings" in directive_content:
                # ê¸°ì¡´ ì„¹ì…˜ ì°¾ì•„ì„œ êµì²´
                parts = directive_content.split("## ğŸ”„ Recent Learnings")
                if len(parts) == 2:
                    # ë‹¤ìŒ ì„¹ì…˜ê¹Œì§€ ë˜ëŠ” íŒŒì¼ ëê¹Œì§€
                    after_section = parts[1]
                    next_section_pos = after_section.find("\n## ")
                    if next_section_pos != -1:
                        directive_content = parts[0] + learning_section + after_section[next_section_pos:]
                    else:
                        directive_content = parts[0] + learning_section
            else:
                # íŒŒì¼ ëì— ì¶”ê°€
                directive_content += learning_section

            # íŒŒì¼ ì €ì¥
            with open(self.directive_file, 'w', encoding='utf-8') as f:
                f.write(directive_content)

            logger.info(f"âœ“ Updated directive with {len(high_severity)} learnings")
            return True

        except Exception as e:
            logger.error(f"Error updating directive: {e}")
            return False

    def run_self_annealing_cycle(self):
        """ìê°€ ê°œì„  ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("Starting self-annealing optimization cycle...")

        # 1. ë¹„íš¨ìœ¨ì„± ë¶„ì„
        inefficiencies = self.analyze_inefficiencies()

        # 2. í•™ìŠµ ìƒì„±
        learnings = self.generate_learnings(inefficiencies)

        # 3. í•™ìŠµ ì €ì¥
        if learnings:
            self.save_learnings(learnings)

        # 4. Directive ì—…ë°ì´íŠ¸ (í•„ìš”ì‹œ)
        updated = self.update_directive_if_needed(learnings)

        # 5. ê²°ê³¼ ë¦¬í¬íŠ¸
        result = {
            'timestamp': datetime.now().isoformat(),
            'inefficiencies_found': {
                'large_prompts': len(inefficiencies.get('large_prompts', [])),
                'repeated_queries': len(inefficiencies.get('repeated_queries', [])),
                'missed_opportunities': len(inefficiencies.get('missed_opportunities', []))
            },
            'learnings_generated': len(learnings),
            'directive_updated': updated
        }

        logger.info("âœ“ Self-annealing cycle completed")
        logger.info(f"  Large prompts: {result['inefficiencies_found']['large_prompts']}")
        logger.info(f"  Repeated queries: {result['inefficiencies_found']['repeated_queries']}")
        logger.info(f"  Learnings: {result['learnings_generated']}")
        logger.info(f"  Directive updated: {result['directive_updated']}")

        return result

    def get_learning_summary(self) -> List[Dict]:
        """í•™ìŠµ ìš”ì•½ ë°˜í™˜"""
        if not self.learning_file.exists():
            return []

        try:
            with open(self.learning_file, 'r', encoding='utf-8') as f:
                learnings = json.load(f)

            # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
            by_category = defaultdict(list)
            for learning in learnings:
                by_category[learning['category']].append(learning)

            summary = []
            for category, items in by_category.items():
                summary.append({
                    'category': category,
                    'count': len(items),
                    'latest': items[-1] if items else None
                })

            return summary

        except Exception as e:
            logger.error(f"Error reading learning summary: {e}")
            return []


def main():
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    import sys

    optimizer = SelfAnnealingOptimizer()

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "run":
            # ìê°€ ê°œì„  ì‚¬ì´í´ ì‹¤í–‰
            result = optimizer.run_self_annealing_cycle()
            print("\n" + "="*60)
            print("ğŸ”„ SELF-ANNEALING OPTIMIZATION CYCLE")
            print("="*60)
            print(f"Large prompts found:     {result['inefficiencies_found']['large_prompts']}")
            print(f"Repeated queries:        {result['inefficiencies_found']['repeated_queries']}")
            print(f"Learnings generated:     {result['learnings_generated']}")
            print(f"Directive updated:       {'Yes' if result['directive_updated'] else 'No'}")
            print("="*60 + "\n")

        elif command == "summary":
            # í•™ìŠµ ìš”ì•½
            summary = optimizer.get_learning_summary()
            print("\n" + "="*60)
            print("ğŸ“š LEARNING SUMMARY")
            print("="*60)
            for item in summary:
                print(f"\n{item['category'].replace('_', ' ').title()}: {item['count']} learnings")
                if item['latest']:
                    print(f"  Latest: {item['latest']['observation']}")
            print("\n" + "="*60 + "\n")

        elif command == "analyze":
            # ë¶„ì„ë§Œ ì‹¤í–‰
            inefficiencies = optimizer.analyze_inefficiencies()
            print("\n" + "="*60)
            print("ğŸ” INEFFICIENCY ANALYSIS")
            print("="*60)
            print(f"Large prompts:           {len(inefficiencies['large_prompts'])}")
            print(f"Repeated queries:        {len(inefficiencies['repeated_queries'])}")
            print(f"Missed opportunities:    {len(inefficiencies['missed_opportunities'])}")
            print("="*60 + "\n")

    else:
        print("Usage:")
        print("  python self_annealing_optimizer.py run        # Run full cycle")
        print("  python self_annealing_optimizer.py summary    # Show learning summary")
        print("  python self_annealing_optimizer.py analyze    # Analyze only")


if __name__ == "__main__":
    main()
