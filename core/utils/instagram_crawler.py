#!/usr/bin/env python3
"""
Instagram Crawler for WOOHWAHAE
@woosunhokr ê³„ì • ì½˜í…ì¸  ìë™ ìˆ˜ì§‘ ë° ë¶„ë¥˜

Features:
- Instaloaderë¥¼ ì‚¬ìš©í•œ ê³µê°œ í”„ë¡œí•„ í¬ë¡¤ë§
- í¬ìŠ¤íŠ¸, ìº¡ì…˜, í•´ì‹œíƒœê·¸, ì´ë¯¸ì§€ ìë™ ìˆ˜ì§‘
- AI ê¸°ë°˜ ì½˜í…ì¸  ë¶„ë¥˜ (7ê°œ ì„¹ì…˜)
- ë¡œì»¬ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€

Author: WOOHWAHAE System
Created: 2026-02-17
"""

import os
import json
import time
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
import logging

# Instaloader for Instagram crawling
try:
    import instaloader
    INSTALOADER_AVAILABLE = True
except ImportError:
    INSTALOADER_AVAILABLE = False
    print("âš ï¸  instaloader not installed. Run: pip install instaloader")

# Image analysis
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

logger = logging.getLogger(__name__)


class InstagramCrawler:
    """
    Instagram Crawler for @woosunhokr

    ìë™ìœ¼ë¡œ í¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³  WOOHWAHAE 7ê°œ ì„¹ì…˜ì— ë¶„ë¥˜
    """

    def __init__(self, username: str = "woosunhokr", cache_dir: Optional[str] = None):
        """
        Initialize Instagram Crawler

        Args:
            username: Instagram username to crawl
            cache_dir: Directory for caching (default: project_root/data/instagram_cache)
        """
        if not INSTALOADER_AVAILABLE:
            raise ImportError("instaloader required: pip install instaloader")

        self.username = username
        self.loader = instaloader.Instaloader(
            download_pictures=True,
            download_videos=False,
            download_video_thumbnails=False,
            download_geotags=False,
            download_comments=False,
            save_metadata=True,
            compress_json=False,
            post_metadata_txt_pattern="",  # JSONë§Œ ì €ì¥
            max_connection_attempts=3
        )

        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            project_root = Path(__file__).parent.parent.parent
            self.cache_dir = project_root / "data" / "instagram_cache" / username

        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ìˆ˜ì§‘ ê¸°ë¡
        self.history_file = self.cache_dir / "crawl_history.json"
        self.history = self._load_history()

        logger.info("Instagram Crawler initialized for @%s", username)

    def _load_history(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê¸°ë¡ ë¡œë“œ"""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (OSError, ValueError):
                pass
        return {
            'last_crawl': None,
            'posts_collected': {},
            'total_posts': 0
        }

    def _save_history(self):
        """í¬ë¡¤ë§ ê¸°ë¡ ì €ì¥"""
        self.history['last_crawl'] = datetime.now().isoformat()
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def crawl_recent_posts(self, limit: int = 10, days_back: int = 7) -> List[Dict[str, Any]]:
        """
        ìµœê·¼ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§

        Args:
            limit: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            days_back: ë©°ì¹  ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€

        Returns:
            ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        print(f"[Crawler] @{self.username} ìµœê·¼ {days_back}ì¼ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")

        try:
            # í”„ë¡œí•„ ë¡œë“œ
            profile = instaloader.Profile.from_username(self.loader.context, self.username)
            posts_collected = []
            cutoff_date = datetime.now() - timedelta(days=days_back)

            # í¬ìŠ¤íŠ¸ ìˆœíšŒ
            for post in profile.get_posts():
                # ë‚ ì§œ ì²´í¬
                if post.date < cutoff_date:
                    break

                # ì¤‘ë³µ ì²´í¬
                post_id = str(post.shortcode)
                if post_id in self.history['posts_collected']:
                    logger.debug("Skip duplicate: %s", post_id)
                    continue

                # í¬ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
                post_data = self._extract_post_data(post)

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                if post.is_video:
                    post_data['media_type'] = 'video'
                else:
                    post_data['media_type'] = 'image'
                    # ì´ë¯¸ì§€ ì €ì¥
                    image_path = self._download_image(post)
                    if image_path:
                        post_data['local_image_path'] = str(image_path)

                # ì„¹ì…˜ ë¶„ë¥˜ íŒíŠ¸ ì¶”ê°€
                post_data['section_hints'] = self._classify_post(post_data)

                # ìºì‹œ ì €ì¥
                self._save_post_cache(post_id, post_data)

                # ê¸°ë¡ ì—…ë°ì´íŠ¸
                self.history['posts_collected'][post_id] = {
                    'collected_at': datetime.now().isoformat(),
                    'caption_preview': post_data['caption'][:100]
                }

                posts_collected.append(post_data)

                print(f"[Crawler] ìˆ˜ì§‘: {post_id} - {post_data['caption'][:50]}...")

                if len(posts_collected) >= limit:
                    break

                # Rate limiting
                time.sleep(2)

            self.history['total_posts'] = len(self.history['posts_collected'])
            self._save_history()

            print(f"[Crawler] ìˆ˜ì§‘ ì™„ë£Œ: {len(posts_collected)}ê°œ í¬ìŠ¤íŠ¸")
            return posts_collected

        except Exception as e:
            logger.error("Crawling failed: %s", e)
            return []

    def _extract_post_data(self, post) -> Dict[str, Any]:
        """í¬ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        return {
            'post_id': post.shortcode,
            'url': f"https://www.instagram.com/p/{post.shortcode}/",
            'caption': post.caption if post.caption else "",
            'hashtags': list(post.caption_hashtags) if post.caption_hashtags else [],
            'mentions': list(post.caption_mentions) if post.caption_mentions else [],
            'date': post.date.isoformat(),
            'likes': post.likes,
            'comments': post.comments,
            'is_video': post.is_video,
            'location': post.location.name if post.location else None,
            'accessibility_caption': post.accessibility_caption,
            'crawled_at': datetime.now().isoformat()
        }

    def _download_image(self, post) -> Optional[Path]:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥"""
        try:
            image_dir = self.cache_dir / "images"
            image_dir.mkdir(exist_ok=True)

            # ì´ë¯¸ì§€ íŒŒì¼ëª…
            image_filename = f"{post.shortcode}.jpg"
            image_path = image_dir / image_filename

            if image_path.exists():
                return image_path

            # Instaloaderë¡œ ë‹¤ìš´ë¡œë“œ
            self.loader.download_pic(
                filename=str(image_path.with_suffix('')),
                url=post.url,
                mtime=post.date
            )

            return image_path

        except Exception as e:
            logger.error("Image download failed: %s", e)
            return None

    def _classify_post(self, post_data: Dict[str, Any]) -> Dict[str, float]:
        """
        í¬ìŠ¤íŠ¸ë¥¼ 7ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜ (íœ´ë¦¬ìŠ¤í‹±)

        Returns:
            ì„¹ì…˜ë³„ í™•ë¥  ì ìˆ˜
        """
        caption = post_data['caption'].lower()
        hashtags = [tag.lower() for tag in post_data.get('hashtags', [])]
        all_text = caption + ' ' + ' '.join(hashtags)

        scores = {
            'about': 0.0,
            'archive': 0.0,
            'shop': 0.0,
            'service': 0.0,
            'playlist': 0.0,
            'project': 0.0,
            'photography': 0.0
        }

        # Service (í—¤ì–´ ê´€ë ¨)
        hair_keywords = ['í—¤ì–´', 'hair', 'íŒ', 'perm', 'ì»·', 'cut', 'ìŠ¤íƒ€ì¼', 'style', 'ë¯¸ìš©']
        for keyword in hair_keywords:
            if keyword in all_text:
                scores['service'] += 20.0

        # Archive (ë§¤ê±°ì§„, ì—ì„¸ì´)
        archive_keywords = ['archive', 'ì•„ì¹´ì´ë¸Œ', 'magazine', 'ë§¤ê±°ì§„', 'ê¸€', 'ì—ì„¸ì´', 'ìƒê°']
        for keyword in archive_keywords:
            if keyword in all_text:
                scores['archive'] += 20.0

        # Shop (ì œí’ˆ ì–¸ê¸‰)
        shop_keywords = ['ì´ì†', 'aesop', 'ì œí’ˆ', 'product', 'ë°€ë³¸', 'milbon']
        for keyword in shop_keywords:
            if keyword in all_text:
                scores['shop'] += 15.0

        # Project (í˜‘ì—…)
        project_keywords = ['í˜‘ì—…', 'collaboration', 'í”„ë¡œì íŠ¸', 'project', 'with']
        for keyword in project_keywords:
            if keyword in all_text:
                scores['project'] += 15.0

        # Photography (ë¹„ì£¼ì–¼ ì¤‘ì‹¬)
        if post_data.get('media_type') == 'image' and len(caption) < 100:
            scores['photography'] += 25.0

        # Playlist (ìŒì•…)
        music_keywords = ['ìŒì•…', 'music', 'playlist', 'í”Œë ˆì´ë¦¬ìŠ¤íŠ¸', 'bgm']
        for keyword in music_keywords:
            if keyword in all_text:
                scores['playlist'] += 20.0

        # About (ì² í•™ì )
        philosophy_keywords = ['slowlife', 'ìŠ¬ë¡œìš°ë¼ì´í”„', 'ìƒê°', 'ì² í•™', 'philosophy', 'woohwahae']
        for keyword in philosophy_keywords:
            if keyword in all_text:
                scores['about'] += 15.0

        # ì •ê·œí™” (í•©ì´ 100ì´ ë˜ë„ë¡)
        total = sum(scores.values())
        if total > 0:
            for key in scores:
                scores[key] = round((scores[key] / total) * 100, 2)

        return scores

    def _save_post_cache(self, post_id: str, post_data: Dict[str, Any]):
        """í¬ìŠ¤íŠ¸ ë°ì´í„° ìºì‹œ ì €ì¥"""
        post_file = self.cache_dir / f"{post_id}.json"
        with open(post_file, 'w', encoding='utf-8') as f:
            json.dump(post_data, f, ensure_ascii=False, indent=2)

    def get_cached_posts(self) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ëª¨ë“  í¬ìŠ¤íŠ¸ ë°˜í™˜"""
        posts = []
        for json_file in self.cache_dir.glob("*.json"):
            if json_file.name != "crawl_history.json":
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        posts.append(json.load(f))
                except (OSError, ValueError):
                    continue
        return posts

    def analyze_hashtag_trends(self) -> Dict[str, int]:
        """í•´ì‹œíƒœê·¸ íŠ¸ë Œë“œ ë¶„ì„"""
        hashtag_counts = {}
        posts = self.get_cached_posts()

        for post in posts:
            for tag in post.get('hashtags', []):
                tag_lower = tag.lower()
                hashtag_counts[tag_lower] = hashtag_counts.get(tag_lower, 0) + 1

        # ë¹ˆë„ìˆœ ì •ë ¬
        return dict(sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True))

    def get_section_distribution(self) -> Dict[str, float]:
        """ì„¹ì…˜ë³„ ì½˜í…ì¸  ë¶„í¬ ë¶„ì„"""
        section_totals = {
            'about': 0.0,
            'archive': 0.0,
            'shop': 0.0,
            'service': 0.0,
            'playlist': 0.0,
            'project': 0.0,
            'photography': 0.0
        }

        posts = self.get_cached_posts()
        for post in posts:
            hints = post.get('section_hints', {})
            for section, score in hints.items():
                section_totals[section] += score

        # í‰ê·  ê³„ì‚°
        if posts:
            for section in section_totals:
                section_totals[section] = round(section_totals[section] / len(posts), 2)

        return section_totals


# ================== Standalone Execution ==================

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Instagram Crawler for WOOHWAHAE')
    parser.add_argument('--username', default='woosunhokr', help='Instagram username')
    parser.add_argument('--limit', type=int, default=10, help='Max posts to crawl')
    parser.add_argument('--days', type=int, default=7, help='Days back to crawl')
    parser.add_argument('--analyze', action='store_true', help='Analyze cached posts')

    args = parser.parse_args()

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = InstagramCrawler(username=args.username)

    if args.analyze:
        # ìºì‹œëœ í¬ìŠ¤íŠ¸ ë¶„ì„
        print("\n" + "="*50)
        print("Instagram Content Analysis")
        print("="*50 + "\n")

        # ì„¹ì…˜ ë¶„í¬
        print("ğŸ“Š Section Distribution:")
        distribution = crawler.get_section_distribution()
        for section, avg_score in sorted(distribution.items(), key=lambda x: x[1], reverse=True):
            bar = "â–ˆ" * int(avg_score / 5)
            print(f"  {section:12} {bar} {avg_score}%")

        # í•´ì‹œíƒœê·¸ íŠ¸ë Œë“œ
        print("\nğŸ·ï¸  Top Hashtags:")
        trends = crawler.analyze_hashtag_trends()
        for tag, count in list(trends.items())[:10]:
            print(f"  #{tag:20} {count}íšŒ")

        # ìºì‹œ ìƒíƒœ
        cached_posts = crawler.get_cached_posts()
        print(f"\nğŸ“¦ Cached Posts: {len(cached_posts)}ê°œ")

    else:
        # ìƒˆ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§
        print(f"\nğŸ” Crawling @{args.username}...")
        print(f"   ìµœëŒ€ {args.limit}ê°œ, ìµœê·¼ {args.days}ì¼")
        print("-" * 50)

        posts = crawler.crawl_recent_posts(limit=args.limit, days_back=args.days)

        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ í¬ìŠ¤íŠ¸")

        if posts:
            print("\nìµœê·¼ í¬ìŠ¤íŠ¸:")
            for post in posts[:3]:
                print(f"\n  ğŸ“ {post['post_id']}")
                print(f"     {post['caption'][:100]}...")
                print(f"     ì„¹ì…˜ íŒíŠ¸: {post.get('section_hints', {})}")