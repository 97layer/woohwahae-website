#!/usr/bin/env python3
"""
Ralph Loop - STAP Validation Engine
97layerOS í’ˆì§ˆ ë³´ì¦ ì‹œìŠ¤í…œ

ì² í•™:
- "ì™„ë²½ì£¼ì˜ ë§ˆë¹„"ë¥¼ ê·¹ë³µí•˜ë˜, í’ˆì§ˆì€ ê°•ì œí•œë‹¤
- 72ì‹œê°„ ê·œì¹™: ë¶ˆì™„ì „í•œ ì™„ë£Œ > ì™„ë²½í•œ ì§€ì—°
- STAP 4ë‹¨ê³„ ê²€ì¦ìœ¼ë¡œ ìµœì†Œ í’ˆì§ˆ ë³´ì¥

STAP Protocol:
  S (Stop)    - ê²°ê³¼ë¬¼ì„ ë©ˆì¶”ê³  í‰ê°€ ì‹œì‘
  T (Task)    - ì›ë˜ ëª©í‘œ/ì˜ë„ì™€ ëŒ€ì¡°
  A (Assess)  - í’ˆì§ˆ ì ìˆ˜í™” (0-100)
  P (Process) - Pass/Revise/Archive ê²°ì •

Author: 97layerOS Technical Director
Created: 2026-02-16
"""

import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
import json

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class RalphLoop:
    """
    Ralph Loop STAP Validation Engine

    ì™„ë²½ì£¼ì˜ ë§ˆë¹„ì™€ ë‚®ì€ í’ˆì§ˆ ì‚¬ì´ì˜ ê· í˜•ì„ ìœ ì§€í•˜ëŠ” í’ˆì§ˆ ë³´ì¦ ì‹œìŠ¤í…œ
    """

    def __init__(self):
        self.validation_log_path = PROJECT_ROOT / 'knowledge' / 'system' / 'ralph_validations.jsonl'
        self.validation_log_path.parent.mkdir(parents=True, exist_ok=True)

        # í’ˆì§ˆ ì„ê³„ê°’
        self.THRESHOLDS = {
            'pass': 70,          # 70ì  ì´ìƒ: ì¦‰ì‹œ í†µê³¼
            'revise': 50,        # 50-69ì : 1íšŒ ì¬ì‘ì—… ê¶Œì¥
            'archive': 50        # 50ì  ë¯¸ë§Œ: ì•„ì¹´ì´ë¸Œ (í–¥í›„ ì°¸ê³ ìš©)
        }

    def validate(
        self,
        asset_path: str,
        original_task: str,
        content: str,
        asset_type: str = "content",
        metadata: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        STAP ê²€ì¦ ì‹¤í–‰

        Args:
            asset_path: ìì‚° íŒŒì¼ ê²½ë¡œ
            original_task: ì›ë˜ ì‘ì—… ì˜ë„/ëª©í‘œ
            content: ê²€ì¦í•  ì½˜í…ì¸ 
            asset_type: ìì‚° ìœ í˜• (content, code, visual, insight)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            {
                'decision': 'pass' | 'revise' | 'archive',
                'quality_score': 0-100,
                'stap_report': {...},
                'recommendations': [...]
            }
        """
        print(f"\n{'='*70}")
        print(f"ğŸ” Ralph Loop - STAP Validation")
        print(f"{'='*70}")
        print(f"ğŸ“„ Asset: {asset_path}")
        print(f"ğŸ¯ Task: {original_task[:60]}...")

        # S - Stop: ê²°ê³¼ë¬¼ ì¤€ë¹„
        stop_result = self._step_stop(asset_path, content)

        # T - Task: ëª©í‘œ ëŒ€ì¡°
        task_result = self._step_task(original_task, content, asset_type)

        # A - Assess: í’ˆì§ˆ ì ìˆ˜í™”
        assess_result = self._step_assess(content, asset_type, task_result)

        # P - Process: ìµœì¢… ê²°ì •
        process_result = self._step_process(assess_result['quality_score'])

        # í†µí•© ê²°ê³¼
        stap_report = {
            'timestamp': datetime.now().isoformat(),
            'asset_path': asset_path,
            'original_task': original_task,
            'asset_type': asset_type,
            'stop': stop_result,
            'task': task_result,
            'assess': assess_result,
            'process': process_result,
            'metadata': metadata or {}
        }

        # ë¡œê·¸ ì €ì¥
        self._log_validation(stap_report)

        # ê²°ê³¼ ë°˜í™˜
        result = {
            'decision': process_result['decision'],
            'quality_score': assess_result['quality_score'],
            'stap_report': stap_report,
            'recommendations': process_result['recommendations']
        }

        # ì¶œë ¥
        self._print_result(result)

        return result

    def _step_stop(self, asset_path: str, content: str) -> Dict[str, Any]:
        """S - Stop: ê²°ê³¼ë¬¼ ë©ˆì¶”ê³  ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        word_count = len(content.split())
        char_count = len(content)
        has_structure = any(marker in content for marker in ['#', '##', '```', '---'])

        return {
            'asset_path': asset_path,
            'word_count': word_count,
            'char_count': char_count,
            'has_structure': has_structure,
            'timestamp': datetime.now().isoformat()
        }

    def _step_task(self, original_task: str, content: str, asset_type: str) -> Dict[str, Any]:
        """T - Task: ì›ë˜ ëª©í‘œì™€ ëŒ€ì¡°"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        task_keywords = set(original_task.lower().split())
        content_keywords = set(content.lower().split())

        # í‚¤ì›Œë“œ ì¼ì¹˜ìœ¨
        common_keywords = task_keywords & content_keywords
        if len(task_keywords) > 0:
            keyword_match_rate = len(common_keywords) / len(task_keywords)
        else:
            keyword_match_rate = 0.0

        # ì‘ì—… ìœ í˜•ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸
        checklist = self._get_type_checklist(asset_type, content)

        return {
            'task_keywords': list(task_keywords)[:10],  # ìƒìœ„ 10ê°œë§Œ
            'keyword_match_rate': round(keyword_match_rate * 100, 2),
            'checklist': checklist,
            'alignment': 'high' if keyword_match_rate > 0.5 else 'medium' if keyword_match_rate > 0.3 else 'low'
        }

    def _get_type_checklist(self, asset_type: str, content: str) -> Dict[str, bool]:
        """ìì‚° ìœ í˜•ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸"""
        if asset_type == "content":
            return {
                'has_title': content.startswith('#'),
                'has_sections': '##' in content,
                'has_conclusion': any(word in content.lower() for word in ['ê²°ë¡ ', 'ìš”ì•½', 'conclusion', 'summary']),
                'sufficient_length': len(content.split()) >= 100
            }
        elif asset_type == "code":
            return {
                'has_docstring': '"""' in content or "'''" in content,
                'has_comments': '#' in content and not content.strip().startswith('#'),
                'has_functions': 'def ' in content,
                'has_main': '__main__' in content
            }
        elif asset_type == "visual":
            return {
                'has_description': len(content) > 50,
                'has_context': '**' in content or '__' in content,
                'has_metadata': 'From:' in content or 'Type:' in content
            }
        else:  # insight
            return {
                'has_context': len(content) > 100,
                'has_structure': '##' in content or '-' in content,
                'has_actionable': any(word in content for word in ['í–‰ë™', 'action', 'ë‹¤ìŒ', 'next'])
            }

    def _step_assess(self, content: str, asset_type: str, task_result: Dict) -> Dict[str, Any]:
        """A - Assess: í’ˆì§ˆ ì ìˆ˜í™” (0-100)"""
        scores = []

        # 1. ì‘ì—… ì •ë ¬ë„ (Task Alignment) - 30ì 
        keyword_match = task_result['keyword_match_rate']
        scores.append(('task_alignment', keyword_match * 0.3))

        # 2. êµ¬ì¡° ì™„ì„±ë„ (Structure) - 30ì 
        checklist = task_result['checklist']
        structure_score = (sum(checklist.values()) / len(checklist)) * 30
        scores.append(('structure', structure_score))

        # 3. ë‚´ìš© ì¶©ì‹¤ë„ (Content Quality) - 25ì 
        word_count = len(content.split())
        if word_count >= 300:
            content_score = 25
        elif word_count >= 150:
            content_score = 20
        elif word_count >= 50:
            content_score = 15
        else:
            content_score = 10
        scores.append(('content_quality', content_score))

        # 4. ê°€ë…ì„± (Readability) - 15ì 
        has_paragraphs = '\n\n' in content
        has_formatting = any(marker in content for marker in ['**', '__', '`', '```'])
        readability_score = (has_paragraphs * 8) + (has_formatting * 7)
        scores.append(('readability', readability_score))

        # ì´ì  ê³„ì‚°
        total_score = sum(score for _, score in scores)

        return {
            'quality_score': round(total_score, 2),
            'breakdown': dict(scores),
            'grade': self._get_grade(total_score)
        }

    def _get_grade(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 90:
            return 'A+'
        elif score >= 80:
            return 'A'
        elif score >= 70:
            return 'B+'
        elif score >= 60:
            return 'B'
        elif score >= 50:
            return 'C'
        else:
            return 'D'

    def _step_process(self, quality_score: float) -> Dict[str, Any]:
        """P - Process: Pass/Revise/Archive ê²°ì •"""
        if quality_score >= self.THRESHOLDS['pass']:
            decision = 'pass'
            action = 'âœ… í†µê³¼ - ì¦‰ì‹œ ë°œí–‰ ê°€ëŠ¥'
            recommendations = [
                'í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡±',
                'ìì‚°ìœ¼ë¡œ ë“±ë¡ ë° ë°œí–‰'
            ]
        elif quality_score >= self.THRESHOLDS['revise']:
            decision = 'revise'
            action = 'ğŸ”„ ì¬ì‘ì—… ê¶Œì¥ - 1íšŒ ê°œì„  í›„ ì¬ê²€ì¦'
            recommendations = [
                'êµ¬ì¡° ê°•í™” (ì„¹ì…˜, ì†Œì œëª© ì¶”ê°€)',
                'ë‚´ìš© ë³´ì¶© (ìµœì†Œ 300ë‹¨ì–´ ëª©í‘œ)',
                'ê°€ë…ì„± ê°œì„  (ë‹¨ë½, í¬ë§·íŒ…)',
                'ì‘ì—… ì˜ë„ì™€ ì •ë ¬ í™•ì¸'
            ]
        else:
            decision = 'archive'
            action = 'ğŸ“¦ ì•„ì¹´ì´ë¸Œ - í˜„ì¬ëŠ” ë³´ë¥˜, í–¥í›„ ì°¸ê³ ìš©'
            recommendations = [
                'í˜„ì¬ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬',
                'knowledge/archive/low_quality/ë¡œ ì´ë™',
                'í–¥í›„ ê°œì„  ë˜ëŠ” íê¸° ê²€í† '
            ]

        return {
            'decision': decision,
            'action': action,
            'recommendations': recommendations,
            'threshold_used': self.THRESHOLDS
        }

    def _log_validation(self, stap_report: Dict):
        """ê²€ì¦ ê²°ê³¼ ë¡œê·¸ ì €ì¥ (JSONL)"""
        with open(self.validation_log_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(stap_report, ensure_ascii=False) + '\n')

    def _print_result(self, result: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Ralph Loop ê²€ì¦ ê²°ê³¼")
        print(f"{'='*70}")
        print(f"â­ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/100")
        print(f"   ë“±ê¸‰: {result['stap_report']['assess']['grade']}")
        print(f"\nğŸ¯ ìµœì¢… ê²°ì •: {result['decision'].upper()}")
        print(f"   {result['stap_report']['process']['action']}")
        print(f"\nğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
        for i, rec in enumerate(result['recommendations'], 1):
            print(f"   {i}. {rec}")
        print(f"{'='*70}\n")

    def get_validation_history(self, limit: int = 10) -> List[Dict]:
        """ìµœê·¼ ê²€ì¦ ì´ë ¥ ì¡°íšŒ"""
        if not self.validation_log_path.exists():
            return []

        validations = []
        with open(self.validation_log_path, 'r', encoding='utf-8') as f:
            for line in f:
                validations.append(json.loads(line))

        return validations[-limit:]

    def get_statistics(self) -> Dict[str, Any]:
        """ê²€ì¦ í†µê³„"""
        history = self.get_validation_history(limit=1000)

        if not history:
            return {
                'total': 0,
                'pass_rate': 0,
                'avg_score': 0,
                'by_decision': {}
            }

        total = len(history)
        decisions = {}
        scores = []

        for validation in history:
            decision = validation['process']['decision']
            decisions[decision] = decisions.get(decision, 0) + 1
            scores.append(validation['assess']['quality_score'])

        return {
            'total': total,
            'pass_rate': round((decisions.get('pass', 0) / total) * 100, 2),
            'avg_score': round(sum(scores) / len(scores), 2) if scores else 0,
            'by_decision': decisions
        }


def main():
    """CLI í…ŒìŠ¤íŠ¸"""
    import argparse

    parser = argparse.ArgumentParser(description="Ralph Loop STAP Validation")
    parser.add_argument('--test', action='store_true', help='Run test validation')
    parser.add_argument('--stats', action='store_true', help='Show validation statistics')
    args = parser.parse_args()

    loop = RalphLoop()

    if args.stats:
        stats = loop.get_statistics()
        print(f"\nğŸ“Š Ralph Loop í†µê³„")
        print(f"{'='*70}")
        print(f"ì´ ê²€ì¦ íšŸìˆ˜: {stats['total']}")
        print(f"í†µê³¼ìœ¨: {stats['pass_rate']}%")
        print(f"í‰ê·  ì ìˆ˜: {stats['avg_score']}/100")
        print(f"\nê²°ì • ë¶„í¬:")
        for decision, count in stats['by_decision'].items():
            print(f"  {decision}: {count}íšŒ")
        print(f"{'='*70}\n")
        return

    if args.test:
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_content = """# ìŠ¬ë¡œìš° ë¼ì´í”„ì˜ ì‹¤ì²œ

## í•µì‹¬ ì² í•™

ì†ë„ë³´ë‹¤ ë°©í–¥, íš¨ìœ¨ë³´ë‹¤ ë³¸ì§ˆì„ ì„ íƒí•˜ëŠ” ì‚¶ì˜ ë°©ì‹ì…ë‹ˆë‹¤.

### ì‹¤ì²œ ë°©ë²•

1. ì™„ë²½ë³´ë‹¤ ì™„ë£Œë¥¼ ìš°ì„ 
2. 72ì‹œê°„ ê·œì¹™ ì ìš©
3. ê³¼ì •ì˜ í”ì  ë³´ì¡´

## ê²°ë¡ 

ë¶ˆì™„ì „í•¨ì„ ìˆ˜ìš©í•˜ë©° ë‚˜ë‹¤ìš´ ì†ë„ë¡œ ë‚˜ì•„ê°‘ë‹ˆë‹¤.
"""

        result = loop.validate(
            asset_path="tests/test_slow_life.md",
            original_task="ìŠ¬ë¡œìš° ë¼ì´í”„ ì² í•™ì„ ì„¤ëª…í•˜ëŠ” ì½˜í…ì¸  ì‘ì„±",
            content=test_content,
            asset_type="content"
        )

        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"ê²°ì •: {result['decision']}")
        print(f"ì ìˆ˜: {result['quality_score']}/100")


if __name__ == "__main__":
    main()
