#!/usr/bin/env python3
"""
Gardener â€” 97layerOS ìê°€ì§„í™” ì—ì´ì „íŠ¸

ë§¤ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰. ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œìŠ¤í…œì„ ì§„í™”ì‹œí‚¨ë‹¤.

ìˆ˜ì • ê¶Œí•œ 3ë‹¨ê³„:
  FROZEN  â€” ì ˆëŒ€ ë¶ˆê°€ (IDENTITY.md, CD.md)
  PROPOSE â€” ìˆœí˜¸ ìŠ¹ì¸ í›„ ì ìš© (SA/AD/CE.md, intent ê¸°ì¤€)
  AUTO    â€” ìë™ ê°±ì‹  (long_term_memory, QUANTA)

Author: 97layerOS
Updated: 2026-02-16
"""

import os
import sys
import json
import logging
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / '.env')
except ImportError:
    pass

import google.genai as genai

logger = logging.getLogger(__name__)

# â”€â”€ ê¶Œí•œ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FROZEN = {
    # ìˆœí˜¸ì˜ ë³¸ì§ˆ â€” ì ˆëŒ€ ë¶ˆê°€
    "IDENTITY.md",
    "CD.md",
}

PROPOSE = {
    # ì—ì´ì „íŠ¸ í–‰ë™ ì§€ì¹¨ â€” ìˆœí˜¸ ìŠ¹ì¸ í•„ìš”
    "SA.md",
    "AD.md",
    "CE.md",
}

# AUTO: long_term_memory.json, INTELLIGENCE_QUANTA.md â†’ ê¸°ì¡´ SA/CEê°€ ì´ë¯¸ ì²˜ë¦¬
# GardenerëŠ” ë¶„ì„ + ì œì•ˆë§Œ ë‹´ë‹¹


class Gardener:
    """
    24ì‹œê°„ ì£¼ê¸° ìê°€ì§„í™” ì—ì´ì „íŠ¸.
    ë°ì´í„° ë¶„ì„ â†’ AUTO ê°±ì‹  â†’ PROPOSE í…”ë ˆê·¸ë¨ ì „ì†¡ â†’ ìŠ¹ì¸ ëŒ€ê¸°
    """

    def __init__(self):
        api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEY í•„ìš”")

        self.client = genai.Client(api_key=api_key)
        self._model = 'gemini-2.5-flash'

        self.knowledge_dir = PROJECT_ROOT / 'knowledge'
        self.directives_dir = PROJECT_ROOT / 'directives'
        self.pending_file = self.knowledge_dir / 'system' / 'gardener_pending.json'

        # ëŒ€ê¸° ì¤‘ì¸ ì œì•ˆ ë¡œë“œ
        self.pending: List[Dict] = self._load_pending()

        logger.info("ğŸŒ± Gardener ì´ˆê¸°í™” ì™„ë£Œ")

    # â”€â”€ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _collect_stats(self, days: int = 7) -> Dict:
        """ì§€ë‚œ Nì¼ ë°ì´í„° í†µê³„ ìˆ˜ì§‘"""
        cutoff = datetime.now() - timedelta(days=days)
        stats = {
            'period_days': days,
            'signal_count': 0,
            'sa_analyzed': 0,
            'avg_score': 0,
            'top_themes': [],
            'top_concepts': [],
            'low_score_patterns': [],
        }

        # signals/ ì¹´ìš´íŠ¸
        signals_dir = self.knowledge_dir / 'signals'
        if signals_dir.exists():
            for sf in signals_dir.glob('**/*.json'):
                try:
                    data = json.loads(sf.read_text(encoding='utf-8'))
                    captured = data.get('captured_at', '')
                    if captured:
                        try:
                            dt = datetime.fromisoformat(captured[:19])
                            if dt < cutoff:
                                continue
                        except Exception:
                            pass
                    stats['signal_count'] += 1
                except Exception:
                    pass

        # corpus entriesì—ì„œ SA ë¶„ì„ ì ìˆ˜/í…Œë§ˆ ìˆ˜ì§‘ (signal íŒŒì¼ì—” analysis key ì—†ìŒ)
        scores = []
        theme_counter: Dict[str, int] = {}
        corpus_entries_dir = self.knowledge_dir / 'corpus' / 'entries'

        if corpus_entries_dir.exists():
            for ef in corpus_entries_dir.glob('*.json'):
                try:
                    entry = json.loads(ef.read_text(encoding='utf-8'))
                    indexed = entry.get('indexed_at', '')
                    if indexed:
                        try:
                            dt = datetime.fromisoformat(indexed[:19])
                            if dt < cutoff:
                                continue
                        except Exception:
                            pass
                    stats['sa_analyzed'] += 1
                    score = entry.get('strategic_score', 0)
                    if score:
                        scores.append(score)
                    for theme in entry.get('themes', []):
                        theme_counter[theme] = theme_counter.get(theme, 0) + 1
                except Exception:
                    pass

        if scores:
            stats['avg_score'] = round(sum(scores) / len(scores), 1)
            stats['low_score_patterns'] = [s for s in scores if s < 50]

        stats['top_themes'] = sorted(
            theme_counter.items(), key=lambda x: x[1], reverse=True
        )[:8]

        # long_term_memory ê°œë…
        lm_path = self.knowledge_dir / 'long_term_memory.json'
        if lm_path.exists():
            try:
                lm = json.loads(lm_path.read_text(encoding='utf-8'))
                concepts = lm.get('concepts', {})
                stats['top_concepts'] = sorted(
                    concepts.items(), key=lambda x: x[1], reverse=True
                )[:10]
            except Exception:
                pass

        return stats

    def _load_directive(self, filename: str) -> str:
        """ì—ì´ì „íŠ¸ ì§€ì‹œì–´ ë¡œë“œ"""
        path = self.directives_dir / 'agents' / filename
        if path.exists():
            return path.read_text(encoding='utf-8')
        return ""

    # â”€â”€ ë¶„ì„ + ì œì•ˆ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _analyze_and_propose(self, stats: Dict) -> List[Dict]:
        """
        Geminië¡œ ë°ì´í„° ë¶„ì„ â†’ PROPOSE ëª©ë¡ ìƒì„±
        ê° ì œì•ˆ: {target_file, section, current, proposed, reason}
        """
        proposals = []

        # SA.md ë¶„ì„ â€” SA ì§‘ì¤‘ í…Œë§ˆ ì—…ë°ì´íŠ¸ ì œì•ˆ
        joon_content = self._load_directive('SA.md')
        if joon_content and stats['top_themes']:
            themes_str = ', '.join(f"{t}({c}íšŒ)" for t, c in stats['top_themes'][:5])
            prompt = f"""ë„ˆëŠ” 97layerOS Gardenerë‹¤.

ì§€ë‚œ {stats['period_days']}ì¼ ë°ì´í„°:
- ì‹ í˜¸ ìˆ˜: {stats['signal_count']}ê°œ
- SA ë¶„ì„: {stats['sa_analyzed']}ê°œ
- í‰ê·  ì ìˆ˜: {stats['avg_score']}
- ìƒìœ„ í…Œë§ˆ: {themes_str}
- ìƒìœ„ ê°œë…: {', '.join(k for k, _ in stats['top_concepts'][:5])}

í˜„ì¬ SA.md ì¼ë¶€:
{joon_content[:800]}

ì§ˆë¬¸: ì´ ë°ì´í„°ë¥¼ ë³´ë©´ SA.mdì—ì„œ ì–´ë–¤ ë¶€ë¶„ì„ ë¯¸ì„¸ì¡°ì •í•˜ë©´ ì¢‹ì„ê¹Œ?
- ì§‘ì¤‘í•  í…Œë§ˆ/ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œê°€?
- ë¶„ì„ ê¸°ì¤€ì—ì„œ ë†“ì¹˜ê³  ìˆëŠ” íŒ¨í„´ì´ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "needs_update": true/false,
  "section": "ì—…ë°ì´íŠ¸í•  ì„¹ì…˜ëª…",
  "reason": "ì™œ í•„ìš”í•œì§€ í•œ ë¬¸ì¥",
  "proposed_addition": "ì¶”ê°€/ìˆ˜ì •í•  ë‚´ìš© (2-3ì¤„)"
}}

ê°œì„ ì´ ë¶ˆí•„ìš”í•˜ë©´ needs_update: false.
JSONë§Œ ì¶œë ¥."""

            try:
                resp = self.client.models.generate_content(
                    model=self._model, contents=[prompt]
                )
                text = resp.text.strip()
                import re
                m = re.search(r'\{.*\}', text, re.DOTALL)
                if m:
                    result = json.loads(m.group())
                    if result.get('needs_update'):
                        proposals.append({
                            'id': f"joon_{datetime.now().strftime('%Y%m%d')}",
                            'target_file': 'SA.md',
                            'section': result.get('section', 'ë¶„ì„ ì§‘ì¤‘ ì˜ì—­'),
                            'reason': result.get('reason', ''),
                            'proposed_addition': result.get('proposed_addition', ''),
                            'status': 'pending',
                            'created_at': datetime.now().isoformat(),
                        })
            except Exception as e:
                logger.warning("SA.md ë¶„ì„ ì‹¤íŒ¨: %s", e)

        return proposals

    # â”€â”€ AUTO ê°±ì‹  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _auto_update_quanta(self, stats: Dict):
        """INTELLIGENCE_QUANTA.md ìë™ ì—…ë°ì´íŠ¸"""
        quanta_path = self.knowledge_dir / 'agent_hub' / 'INTELLIGENCE_QUANTA.md'
        if not quanta_path.exists():
            return

        try:
            content = quanta_path.read_text(encoding='utf-8')
            now = datetime.now().strftime('%Y-%m-%d %H:%M')
            themes_str = ', '.join(t for t, _ in stats['top_themes'][:5])
            concepts_str = ', '.join(k for k, _ in stats['top_concepts'][:5])

            # Gardener ì—…ë°ì´íŠ¸ ì„¹ì…˜ ì°¾ì•„ì„œ ê°±ì‹ 
            marker = "## ğŸŒ± Gardener ìë™ ì—…ë°ì´íŠ¸"
            new_section = (
                f"{marker}\n"
                f"ìµœì¢… ì‹¤í–‰: {now}\n"
                f"ë¶„ì„ ê¸°ê°„: {stats['period_days']}ì¼\n"
                f"ì‹ í˜¸ ìˆ˜ì§‘: {stats['signal_count']}ê°œ / SA ë¶„ì„: {stats['sa_analyzed']}ê°œ\n"
                f"í‰ê·  ì „ëµì ìˆ˜: {stats['avg_score']}\n"
                f"ë¶€ìƒ í…Œë§ˆ: {themes_str}\n"
                f"í•µì‹¬ ê°œë…: {concepts_str}\n"
            )

            if marker in content:
                # ê¸°ì¡´ ì„¹ì…˜ êµì²´
                import re
                content = re.sub(
                    rf"{re.escape(marker)}.*?(?=\n##|\Z)",
                    new_section,
                    content,
                    flags=re.DOTALL
                )
            else:
                content += f"\n\n{new_section}"

            quanta_path.write_text(content, encoding='utf-8')
            logger.info("âœ… INTELLIGENCE_QUANTA.md ìë™ ì—…ë°ì´íŠ¸")
        except Exception as e:
            logger.warning("QUANTA ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    def _evolve_concept_memory(self, stats: Dict):
        """
        ê°œë… ì§„í™” ê¸°ë¡ â€” ëŒ€í™”/ì‹ í˜¸ê°€ ìŒ“ì¼ìˆ˜ë¡ ì‚¬ê³ ê°€ ê¹Šì–´ì§€ëŠ” êµ¬ì¡°ì˜ í•µì‹¬.

        ê¸°ì¡´ long_term_memory.jsonì˜ conceptsëŠ” ì¹´ìš´íŠ¸(ìŠ¬ë¡œìš°ë¼ì´í”„: 1)ë§Œ ì¡´ì¬.
        ì´ ë©”ì„œë“œëŠ” Geminiê°€ corpus entryë“¤ì„ ì½ê³  ê° í•µì‹¬ ê°œë…ì´ ì–´ë–»ê²Œ ì‹¬í™”ëëŠ”ì§€
        ì„œìˆ ë¡œ ê¸°ë¡í•œë‹¤. ëª¨ë¸ì´ ë°”ë€Œì–´ë„ ì´ íŒŒì¼ì„ ì½ìœ¼ë©´ ë™ì¼í•œ ì‚¬ê³  ìˆ˜ì¤€ì—ì„œ ì¶œë°œ ê°€ëŠ¥.
        """
        lm_path = self.knowledge_dir / 'long_term_memory.json'
        if not lm_path.exists():
            return

        try:
            lm = json.loads(lm_path.read_text(encoding='utf-8'))
        except Exception:
            return

        # corpus entries ë¡œë“œ (ìµœê·¼ 30ê°œ â€” ì‚¬ê³  íë¦„ íŒŒì•…ìš©)
        corpus_dir = self.knowledge_dir / 'corpus' / 'entries'
        recent_entries = []
        if corpus_dir.exists():
            entry_files = sorted(corpus_dir.glob('*.json'), reverse=True)[:30]
            for f in entry_files:
                try:
                    recent_entries.append(json.loads(f.read_text(encoding='utf-8')))
                except Exception:
                    pass

        if not recent_entries:
            # corpus ë¹„ì–´ìˆìœ¼ë©´ experiencesì—ì„œ ì¶”ì¶œ
            recent_entries = [
                {"summary": e.get("summary", ""), "themes": [], "key_insights": []}
                for e in lm.get("experiences", [])[-20:]
            ]

        if not recent_entries:
            return

        # ìƒìœ„ ê°œë… ëª©ë¡
        concepts = lm.get("concepts", {})
        top_concepts = sorted(concepts.items(), key=lambda x: x[1] if isinstance(x[1], (int, float)) else 0, reverse=True)[:6]
        if not top_concepts:
            return

        # ê¸°ì¡´ concept_evolution ë¡œë“œ
        concept_evolution = lm.get("concept_evolution", {})

        # ê° ìƒìœ„ ê°œë…ì— ëŒ€í•´ ì§„í™” ì„œìˆ  ìƒì„±
        entries_text = ""
        for e in recent_entries[:15]:
            entries_text += f"- {e.get('summary', '')[:120]}\n"

        concepts_str = ", ".join(k for k, _ in top_concepts)

        prompt = f"""ë„ˆëŠ” 97layerOSì˜ ì§€ì‹ íë ˆì´í„°ë‹¤.

ì•„ë˜ëŠ” ìµœê·¼ ìˆ˜ì§‘ëœ ì‹ í˜¸ë“¤ì˜ ìš”ì•½ì´ë‹¤:
{entries_text}

ì´ ì‚¬ëŒì´ ë°˜ë³µì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” í•µì‹¬ ê°œë…ë“¤: {concepts_str}

ê° ê°œë…ì— ëŒ€í•´ ë‹µí•˜ë¼:
1. ì´ ê°œë…ì´ ì´ˆê¸°ì—ëŠ” ì–´ë–¤ ë§¥ë½ì´ì—ˆëŠ”ê°€?
2. ìµœê·¼ ì‹ í˜¸ë“¤ì„ í†µí•´ ì–´ë–»ê²Œ ì‹¬í™”/í™•ì¥ë˜ì—ˆëŠ”ê°€?
3. í˜„ì¬ ì´ ì‚¬ëŒì˜ ì´ ê°œë…ì— ëŒ€í•œ ì‚¬ê³  ìˆ˜ì¤€ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "concept_evolution": {{
    "ê°œë…ëª…": {{
      "current_depth": "í˜„ì¬ ì‚¬ê³  ê¹Šì´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ",
      "trajectory": "ì´ˆê¸° â†’ í˜„ì¬ ë°©í–¥ìœ¼ë¡œ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€",
      "last_updated": "{datetime.now().strftime('%Y-%m-%d')}"
    }}
  }}
}}

ë¶„ì„ ê°€ëŠ¥í•œ ê°œë…ë§Œ í¬í•¨. JSONë§Œ ì¶œë ¥."""

        try:
            response = self.client.models.generate_content(
                model=self._model,
                contents=[prompt]
            )
            import re as re_module
            text = response.text.strip()
            match = re_module.search(r'\{.*\}', text, re_module.DOTALL)
            if not match:
                return

            result = json.loads(match.group())
            new_evolution = result.get("concept_evolution", {})

            # ê¸°ì¡´ ê¸°ë¡ê³¼ ë³‘í•© (ë®ì–´ì“°ì§€ ì•Šê³  ëˆ„ì )
            for concept, data in new_evolution.items():
                if concept not in concept_evolution:
                    concept_evolution[concept] = data
                else:
                    # ê¸°ì¡´ trajectory ë³´ì¡´ + í˜„ì¬ depth ê°±ì‹ 
                    concept_evolution[concept]["current_depth"] = data.get("current_depth", "")
                    concept_evolution[concept]["last_updated"] = data.get("last_updated", "")
                    prev_traj = concept_evolution[concept].get("trajectory", "")
                    new_traj = data.get("trajectory", "")
                    if new_traj and new_traj != prev_traj:
                        concept_evolution[concept]["trajectory"] = new_traj

            lm["concept_evolution"] = concept_evolution
            lm["metadata"]["last_updated"] = datetime.now().strftime('%Y-%m-%dT%H:%M')

            lm_path.write_text(json.dumps(lm, ensure_ascii=False, indent=2), encoding='utf-8')
            logger.info("ğŸ§  ê°œë… ì§„í™” ê¸°ë¡ ê°±ì‹ : %dê°œ ê°œë…", len(new_evolution))

        except Exception as e:
            logger.warning("ê°œë… ì§„í™” ê¸°ë¡ ì‹¤íŒ¨: %s", e)

    def _update_quanta_with_growth(self, stats: Dict):
        """
        INTELLIGENCE_QUANTA.mdë¥¼ ìƒíƒœ ìŠ¤ëƒ…ìƒ·ì´ ì•„ë‹Œ ì‚¬ê³  ì„±ì¥ ì¼ì§€ë¡œ ê°±ì‹ .
        ì–´ë–¤ ëª¨ë¸ì´ ì½ì–´ë„ í˜„ì¬ ì‚¬ê³  ìˆ˜ì¤€ì„ ì¦‰ì‹œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡.
        """
        quanta_path = self.knowledge_dir / 'agent_hub' / 'INTELLIGENCE_QUANTA.md'
        lm_path = self.knowledge_dir / 'long_term_memory.json'

        if not quanta_path.exists():
            return

        try:
            # concept_evolution ë¡œë“œ
            concept_evolution = {}
            if lm_path.exists():
                lm = json.loads(lm_path.read_text(encoding='utf-8'))
                concept_evolution = lm.get("concept_evolution", {})

            content = quanta_path.read_text(encoding='utf-8')
            now = datetime.now().strftime('%Y-%m-%d %H:%M')
            themes_str = ', '.join(t for t, _ in stats['top_themes'][:5]) or 'ì—†ìŒ'

            # ê°œë… ì§„í™” ìš”ì•½ í…ìŠ¤íŠ¸
            evolution_lines = ""
            for concept, data in list(concept_evolution.items())[:4]:
                depth = data.get("current_depth", "")
                if depth:
                    evolution_lines += f"- **{concept}**: {depth}\n"

            if not evolution_lines:
                evolution_lines = "- (ì•„ì§ ì¶©ë¶„í•œ ì‹ í˜¸ ë¯¸ì¶•ì )\n"

            marker = "## ğŸŒ± Gardener ìë™ ì—…ë°ì´íŠ¸"
            new_section = (
                f"{marker}\n"
                f"ìµœì¢… ì‹¤í–‰: {now}\n\n"
                f"**ìˆ˜ì§‘ í˜„í™©** | ì‹ í˜¸: {stats['signal_count']}ê°œ / SAë¶„ì„: {stats['sa_analyzed']}ê°œ / í‰ê· ì ìˆ˜: {stats['avg_score']}\n\n"
                f"**ë¶€ìƒ í…Œë§ˆ** | {themes_str}\n\n"
                f"**ê°œë… ì‚¬ê³  ìˆ˜ì¤€** (ì„¸ì…˜ ê°„ ì—°ì†ì„± ì•µì»¤)\n"
                f"{evolution_lines}\n"
            )

            import re as re_module
            if marker in content:
                content = re_module.sub(
                    rf"{re_module.escape(marker)}.*?(?=\n##|\Z)",
                    new_section,
                    content,
                    flags=re_module.DOTALL
                )
            else:
                content += f"\n\n{new_section}"

            quanta_path.write_text(content, encoding='utf-8')
            logger.info("âœ… INTELLIGENCE_QUANTA.md ì„±ì¥ ì¼ì§€ ê°±ì‹ ")

        except Exception as e:
            logger.warning("QUANTA ì„±ì¥ ê°±ì‹  ì‹¤íŒ¨: %s", e)

    # â”€â”€ ì œì•ˆ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _load_pending(self) -> List[Dict]:
        if self.pending_file.exists():
            try:
                return json.loads(self.pending_file.read_text(encoding='utf-8'))
            except Exception:
                pass
        return []

    def _save_pending(self):
        self.pending_file.parent.mkdir(parents=True, exist_ok=True)
        self.pending_file.write_text(
            json.dumps(self.pending, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )

    def approve_proposal(self, proposal_id: str) -> Tuple[bool, str]:
        """ìˆœí˜¸ ìŠ¹ì¸ â†’ ì‹¤ì œ íŒŒì¼ ìˆ˜ì •"""
        proposal = next((p for p in self.pending if p['id'] == proposal_id), None)
        if not proposal:
            return False, "ì œì•ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

        filename = proposal['target_file']

        # FROZEN ì´ì¤‘ ì²´í¬
        if filename in FROZEN:
            return False, f"ğŸ”’ {filename}ì€ ìˆ˜ì • ë¶ˆê°€ (FROZEN)"

        if filename not in PROPOSE:
            return False, f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼: {filename}"

        # ì‹¤ì œ íŒŒì¼ ìˆ˜ì •
        path = self.directives_dir / 'agents' / filename
        try:
            content = path.read_text(encoding='utf-8')
            section = proposal['section']
            addition = proposal['proposed_addition']
            now = datetime.now().strftime('%Y-%m-%d')

            # ì„¹ì…˜ ì°¾ì•„ì„œ ì¶”ê°€, ì—†ìœ¼ë©´ ëì— ì¶”ê°€
            if f"## {section}" in content:
                insert_point = content.find(f"## {section}") + len(f"## {section}")
                # ë‹¤ìŒ ## ì•ì— ì‚½ì…
                next_section = content.find('\n##', insert_point)
                if next_section > 0:
                    content = (
                        content[:next_section]
                        + f"\n\n<!-- Gardener {now} -->\n{addition}"
                        + content[next_section:]
                    )
                else:
                    content += f"\n\n<!-- Gardener {now} -->\n{addition}"
            else:
                content += f"\n\n## {section}\n<!-- Gardener {now} -->\n{addition}"

            path.write_text(content, encoding='utf-8')

            # pendingì—ì„œ ì œê±°
            self.pending = [p for p in self.pending if p['id'] != proposal_id]
            self._save_pending()

            logger.info("âœ… ìŠ¹ì¸ ì ìš©: %s / %s", filename, section)
            return True, f"âœ… {filename} â€” {section} ì—…ë°ì´íŠ¸ ì™„ë£Œ"

        except Exception as e:
            return False, f"ì ìš© ì‹¤íŒ¨: {e}"

    def reject_proposal(self, proposal_id: str) -> bool:
        """ìˆœí˜¸ ê±°ì ˆ â†’ pendingì—ì„œ ì œê±°"""
        self.pending = [p for p in self.pending if p['id'] != proposal_id]
        self._save_pending()
        return True

    # â”€â”€ ë©”ì¸ ì‚¬ì´í´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _trigger_essay_for_cluster(self, cluster: Dict) -> Optional[str]:
        """
        ì„±ìˆ™í•œ êµ°ì§‘ â†’ CE Agentì—ê²Œ ì—ì„¸ì´ ì‘ì„± ì§€ì‹œ
        Magazine B ë°©ì‹: ë‹¨ì¼ ì‹ í˜¸ê°€ ì•„ë‹Œ êµ°ì§‘ ì „ì²´ë¥¼ RAGí•´ì„œ ì—ì„¸ì´ ì‘ì„±

        Returns: task_id or None
        """
        from core.system.corpus_manager import CorpusManager
        from core.system.queue_manager import QueueManager

        corpus = CorpusManager()
        entries = corpus.get_entries_for_essay(cluster["entry_ids"])

        if not entries:
            return None

        # ì—ì„¸ì´ RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        rag_context = []
        for e in entries:
            rag_context.append({
                "summary": e.get("summary", ""),
                "key_insights": e.get("key_insights", []),
                "themes": e.get("themes", []),
                "captured_at": e.get("captured_at", ""),
                "signal_type": e.get("signal_type", ""),
                "preview": e.get("raw_content_preview", ""),
            })

        payload = {
            "mode": "corpus_essay",
            "content_type": cluster.get("content_type", "archive"),
            "theme": cluster["theme"],
            "entry_count": cluster["entry_count"],
            "rag_context": rag_context,
            "avg_strategic_score": cluster["avg_strategic_score"],
            "time_span_hours": cluster["hours_span"],
            "instruction": (
                f"ì£¼ì œ '{cluster['theme']}'ì— ê´€í•œ {cluster['entry_count']}ê°œì˜ ì‹ í˜¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
                f"ì›ì†ŒìŠ¤ ë©€í‹°ìœ ì¦ˆ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ë¼. "
                f"archive_essay(ë¡±í¼) / instagram_caption(150ì) / "
                f"carousel_slides(3~5ì¥) / telegram_summary(3ì¤„) / pull_quote(1ë¬¸ì¥) "
                f"5ê°œ í¬ë§·ì„ ë™ì‹œì—. ëª¨ë‘ ê°™ì€ ë³¸ì§ˆì—ì„œ íŒŒìƒ."
            ),
        }

        try:
            queue = QueueManager()
            task_id = queue.create_task(
                agent_type="CE",
                task_type="write_corpus_essay",
                payload=payload,
            )
            logger.info(f"ğŸ–Šï¸  ì—ì„¸ì´ íŠ¸ë¦¬ê±°: {cluster['theme']} ({cluster['entry_count']}ê°œ entry) â†’ CE task {task_id}")
            return task_id
        except Exception as e:
            logger.error(f"ì—ì„¸ì´ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {e}")
            return None

    def _check_corpus_clusters(self) -> Dict:
        """
        Corpus êµ°ì§‘ ì„±ìˆ™ë„ ì ê²€ â†’ ìµì€ êµ°ì§‘ ì—ì„¸ì´ íŠ¸ë¦¬ê±°
        """
        try:
            from core.system.corpus_manager import CorpusManager
            corpus = CorpusManager()
            summary = corpus.get_summary()
            ripe = corpus.get_ripe_clusters()

            triggered = []
            for cluster in ripe:
                task_id = self._trigger_essay_for_cluster(cluster)
                if task_id:
                    triggered.append({
                        "theme": cluster["theme"],
                        "entry_count": cluster["entry_count"],
                        "task_id": task_id,
                    })

            logger.info(
                "ğŸ“š Corpus ì ê²€: ì´ %dê°œ entry / êµ°ì§‘ %dê°œ / ì„±ìˆ™ %dê°œ / ì—ì„¸ì´ íŠ¸ë¦¬ê±° %dê°œ",
                summary["total_entries"], summary["total_clusters"],
                summary["ripe_clusters"], len(triggered)
            )

            return {
                "corpus_summary": summary,
                "ripe_clusters": len(ripe),
                "essay_triggered": triggered,
            }
        except Exception as e:
            logger.warning(f"Corpus ì ê²€ ì‹¤íŒ¨: {e}")
            return {"corpus_summary": {}, "ripe_clusters": 0, "essay_triggered": []}

    def _check_revisit_due(self) -> None:
        """ì¬ë°©ë¬¸ ì‹œê¸°ê°€ ëœ ê³ ê° â†’ Telegram ì•Œë¦¼"""
        try:
            from core.modules.ritual import get_ritual_module
            due_clients = get_ritual_module().get_due_clients()
            if not due_clients:
                return

            admin_id = os.getenv('ADMIN_TELEGRAM_ID')
            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
            if not (admin_id and bot_token):
                logger.warning("Telegram í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì • â€” ì¬ë°©ë¬¸ ì•Œë¦¼ ìƒëµ")
                return

            lines = [f"â° <b>ì¬ë°©ë¬¸ ì˜ˆì • ê³ ê° {len(due_clients)}ëª…</b>"]
            for c in due_clients:
                lines.append(f"â€¢ {c['name']} ({c.get('rhythm', 'ë³´í†µ')} ë¦¬ë“¬)")
            msg = "\n".join(lines)

            import httpx
            httpx.post(
                f"https://api.telegram.org/bot{bot_token}/sendMessage",
                json={"chat_id": admin_id, "text": msg, "parse_mode": "HTML"},
                timeout=10,
            )
            logger.info("â° ì¬ë°©ë¬¸ ì•Œë¦¼ ì „ì†¡: %dëª…", len(due_clients))
        except Exception as e:
            logger.warning("ì¬ë°©ë¬¸ ì•Œë¦¼ ì‹¤íŒ¨: %s", e)

    def _record_growth_snapshot(self) -> None:
        """ì›”ë³„ ì„±ì¥ ì§€í‘œë¥¼ Growth Moduleì— ìë™ ê¸°ë¡"""
        try:
            from core.modules.growth import get_growth_module
            period = datetime.now().strftime('%Y-%m')
            gm = get_growth_module()
            gm.auto_count_content(period)
            gm.auto_count_service(period)
            logger.info("Growth snapshot ì €ì¥: %s", period)
        except Exception as e:
            logger.warning("Growth snapshot ì‹¤íŒ¨: %s", e)

    def run_cycle(self, days: int = 7) -> Dict:
        """
        Gardener ë©”ì¸ ì‚¬ì´í´
        Returns: {stats, proposals, corpus_check, auto_updates}
        """
        logger.info("ğŸŒ± Gardener ì‚¬ì´í´ ì‹œì‘ (ì§€ë‚œ %dì¼)", days)

        # 1. ë°ì´í„° ìˆ˜ì§‘
        stats = self._collect_stats(days)
        logger.info(
            "ğŸ“Š ì‹ í˜¸:%d / SAë¶„ì„:%d / í‰ê· ì ìˆ˜:%s",
            stats['signal_count'], stats['sa_analyzed'], stats['avg_score']
        )

        # 2. ê°œë… ì§„í™” ê¸°ë¡ (í•µì‹¬: ëŒ€í™”ê°€ ìŒ“ì¼ìˆ˜ë¡ ì‚¬ê³ ê°€ ê¹Šì–´ì§€ëŠ” êµ¬ì¡°)
        self._evolve_concept_memory(stats)

        # 3. QUANTA ì„±ì¥ ì¼ì§€ ê°±ì‹  (ìƒíƒœ ìŠ¤ëƒ…ìƒ· â†’ ì‚¬ê³  ìˆ˜ì¤€ ì•µì»¤ë¡œ)
        self._update_quanta_with_growth(stats)

        # 4. Corpus êµ°ì§‘ ì„±ìˆ™ë„ ì ê²€ â†’ ìµì€ ê²ƒ ì—ì„¸ì´ íŠ¸ë¦¬ê±° (í•µì‹¬ ì‹ ê·œ)
        corpus_result = self._check_corpus_clusters()

        # 5. Growth Module ì›”ê°„ ì§‘ê³„ ìë™ ê¸°ë¡
        self._record_growth_snapshot()

        # 6. ì¬ë°©ë¬¸ ì‹œê¸° ê³ ê° ì•Œë¦¼
        self._check_revisit_due()

        # 7. PROPOSE ìƒì„± (ì‹ í˜¸ê°€ 10ê°œ ì´ìƒì¼ ë•Œë§Œ)
        new_proposals = []
        if stats['signal_count'] >= 10:
            new_proposals = self._analyze_and_propose(stats)
            if new_proposals:
                self.pending.extend(new_proposals)
                self._save_pending()
                logger.info("ğŸ“ ìƒˆ ì œì•ˆ %dê°œ ìƒì„±", len(new_proposals))
        else:
            logger.info("â­ï¸  ì‹ í˜¸ ë¶€ì¡± (%dê°œ) â€” ì œì•ˆ ìƒëµ", stats['signal_count'])

        return {
            'stats': stats,
            'new_proposals': new_proposals,
            'pending_count': len(self.pending),
            'corpus': corpus_result,
        }

    def format_telegram_report(self, result: Dict) -> str:
        """í…”ë ˆê·¸ë¨ ì „ì†¡ìš© ë¦¬í¬íŠ¸ í¬ë§·"""
        stats = result['stats']
        proposals = result['new_proposals']

        themes = ', '.join(f"{t}" for t, _ in stats['top_themes'][:4]) or 'ì—†ìŒ'
        concepts = ', '.join(k for k, _ in stats['top_concepts'][:4]) or 'ì—†ìŒ'

        lines = [
            f"ğŸŒ± <b>Gardener ì£¼ê°„ ë¦¬í¬íŠ¸</b>",
            f"",
            f"<b>ì§€ë‚œ {stats['period_days']}ì¼ í˜„í™©</b>",
            f"ì‹ í˜¸ ìˆ˜ì§‘: {stats['signal_count']}ê°œ",
            f"SA ë¶„ì„: {stats['sa_analyzed']}ê°œ",
            f"í‰ê·  ì „ëµì ìˆ˜: {stats['avg_score']}",
            f"",
            f"<b>ë¶€ìƒ í…Œë§ˆ</b>",
            f"{themes}",
            f"",
            f"<b>í•µì‹¬ ê°œë…</b>",
            f"{concepts}",
        ]

        if proposals:
            lines += ["", f"<b>ì‹œìŠ¤í…œ ê°œì„  ì œì•ˆ {len(proposals)}ê±´</b>"]
            for p in proposals:
                lines.append(f"â€¢ {p['target_file']}: {p['reason']}")
            lines.append("")
            lines.append("ìŠ¹ì¸í•˜ë ¤ë©´ /approve, ê±°ì ˆí•˜ë ¤ë©´ /reject")

        return "\n".join(lines)


# â”€â”€ ìŠ¤ì¼€ì¤„ëŸ¬ (GCP systemdì—ì„œ ì‹¤í–‰) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def run_scheduled(hour: int = 3):
    """ë§¤ì¼ ì§€ì • ì‹œê°ì— ì‹¤í–‰"""
    from core.agents.gardener import Gardener

    gardener = Gardener()

    while True:
        now = datetime.now()
        # ë‹¤ìŒ ì‹¤í–‰ ì‹œê° ê³„ì‚°
        next_run = now.replace(hour=hour, minute=0, second=0, microsecond=0)
        if next_run <= now:
            next_run += timedelta(days=1)

        wait_seconds = (next_run - now).total_seconds()
        logger.info("ğŸŒ± Gardener ëŒ€ê¸° ì¤‘ â€” ë‹¤ìŒ ì‹¤í–‰: %s (%.0fì´ˆ í›„)",
                    next_run.strftime('%m/%d %H:%M'), wait_seconds)

        await asyncio.sleep(wait_seconds)

        try:
            result = gardener.run_cycle(days=7)

            # í…”ë ˆê·¸ë¨ ë¦¬í¬íŠ¸ ì „ì†¡
            admin_id = os.getenv('ADMIN_TELEGRAM_ID')
            bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
            if admin_id and bot_token and result['stats']['signal_count'] > 0:
                import httpx
                msg = gardener.format_telegram_report(result)
                httpx.post(
                    f"https://api.telegram.org/bot{bot_token}/sendMessage",
                    json={
                        'chat_id': admin_id,
                        'text': msg,
                        'parse_mode': 'HTML'
                    },
                    timeout=10
                )
                logger.info("ğŸ“¨ í…”ë ˆê·¸ë¨ ë¦¬í¬íŠ¸ ì „ì†¡ ì™„ë£Œ")

        except Exception as e:
            logger.error("Gardener ì‚¬ì´í´ ì‹¤íŒ¨: %s", e)


if __name__ == '__main__':
    import argparse
    from core.system.env_validator import validate_env
    validate_env("gardener")

    logging.basicConfig(level=logging.INFO, format='%(message)s')

    parser = argparse.ArgumentParser(description='97layerOS Gardener')
    parser.add_argument('--run-now', action='store_true', help='ì¦‰ì‹œ 1íšŒ ì‹¤í–‰')
    parser.add_argument('--days', type=int, default=7, help='ë¶„ì„ ê¸°ê°„ (ê¸°ë³¸: 7ì¼)')
    parser.add_argument('--schedule', action='store_true', help='24ì‹œê°„ ìŠ¤ì¼€ì¤„ ëª¨ë“œ')
    parser.add_argument('--hour', type=int, default=3, help='ì‹¤í–‰ ì‹œê° (ê¸°ë³¸: 3ì‹œ)')
    args = parser.parse_args()

    if args.run_now:
        g = Gardener()
        result = g.run_cycle(days=args.days)
        print(g.format_telegram_report(result))

    elif args.schedule:
        asyncio.run(run_scheduled(hour=args.hour))

    else:
        parser.print_help()
