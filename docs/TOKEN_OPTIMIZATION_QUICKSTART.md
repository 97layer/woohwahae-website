# Token Optimization Quick Start Guide

> **ëª©í‘œ**: AI ì—ì´ì „íŠ¸ê°€ ê°œë°œ ì‘ì—… ì‹œ í† í° ì†Œë¹„ë¥¼ 60-80% ì¤„ì´ëŠ” ë°©ë²•

---

## ğŸ“‹ TL;DR (Too Long; Didn't Read)

```bash
# 1. ì£¼ê°„ ë¦¬í¬íŠ¸ í™•ì¸
python execution/system/weekly_optimization_monitor.py

# 2. ìê°€ ê°œì„  ì‹¤í–‰ (ë§¤ì£¼)
python execution/system/self_annealing_optimizer.py run

# 3. ìºì‹œ ì •ë¦¬ (ì„ íƒ)
python execution/system/token_optimizer.py clear 72
```

**í•µì‹¬ 4ê°€ì§€ ê·œì¹™:**
1. íŒŒì¼ ì „ì²´ ì½ê¸° ê¸ˆì§€ â†’ Grep ë¨¼ì €
2. ìºì‹œ ë¨¼ì € í™•ì¸
3. í° íŒŒì¼ì€ ìŠ¤ë‹ˆí«ë§Œ ì¶”ì¶œ
4. ì˜ì¡´ì„± ë¶„ì„ìœ¼ë¡œ í•„ìš”í•œ íŒŒì¼ë§Œ ì½ê¸°

---

## ğŸš€ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

### 1. TokenOptimizer (ìºì‹± & ìŠ¤ë‹ˆí«)

```python
from execution.system.token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

# ìºì‹œ í™•ì¸
cached = optimizer.get_cached_response(prompt)
if cached:
    return cached  # í† í° 0 ì†Œë¹„

# ìŠ¤ë‹ˆí« ì¶”ì¶œ
if optimizer.should_use_snippet(file_content):
    snippet = optimizer.extract_relevant_snippets(
        file_content,
        keywords=["function_name", "class_name"],
        context_lines=3
    )
    # snippetë§Œ AIì—ê²Œ ì „ë‹¬ â†’ 60-80% í† í° ì ˆì•½
```

### 2. DependencyAnalyzer (êµ¬ì¡° íŒŒì•…)

```python
from execution.system.dependency_analyzer import DependencyAnalyzer

analyzer = DependencyAnalyzer()

# íŒŒì¼ ì „ì²´ ì½ì§€ ì•Šê³  êµ¬ì¡°ë§Œ íŒŒì•…
summary = analyzer.get_file_summary("path/to/file.py")
# ì¶œë ¥: í´ë˜ìŠ¤ëª…, í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜, import ëª©ë¡ë§Œ

# ë³€ê²½ ì˜í–¥ ë²”ìœ„ ë¶„ì„
affected = analyzer.find_affected_files("target_file.py")
# ì˜í–¥ë°›ëŠ” íŒŒì¼ë§Œ ì½ìœ¼ë©´ ë¨
```

### 3. ì£¼ê°„ ëª¨ë‹ˆí„°ë§

```bash
# ìµœì í™” í˜„í™© í™•ì¸
python execution/system/weekly_optimization_monitor.py

# ì¶œë ¥:
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“Š WEEKLY TOKEN OPTIMIZATION REPORT
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Overall Grade: A (Very Good)
# Cache Hit Rate: 61.42%
# Tokens Saved: 245,830
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### 4. ìê°€ ê°œì„  ì‹œìŠ¤í…œ

```bash
# ë¹„íš¨ìœ¨ íŒ¨í„´ ìë™ ê°ì§€ ë° í•™ìŠµ
python execution/system/self_annealing_optimizer.py run

# ì¶œë ¥:
# ğŸ”„ SELF-ANNEALING OPTIMIZATION CYCLE
# Large prompts found:     3
# Repeated queries:        12
# Learnings generated:     2
# Directive updated:       Yes
```

---

## ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ë²„ê·¸ ìˆ˜ì •

**âŒ ê¸°ì¡´ ë°©ì‹ (ë¹„íš¨ìœ¨)**
```python
# íŒŒì¼ ì „ì²´ ì½ê¸° (15,000 tokens)
code = read_file("auth/handler.py")
result = ai.query(f"Find bug in:\n{code}")
```

**âœ… ìµœì í™” ë°©ì‹ (1,500 tokens)**
```python
from execution.system.token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

# 1. ë²„ê·¸ ìœ„ì¹˜ ì°¾ê¸°
grep("error.*authentication", "auth/*.py")  # 100 tokens
# â†’ auth/handler.py:145 ë°œê²¬

# 2. ê´€ë ¨ ë¶€ë¶„ë§Œ ì½ê¸°
code = read_file("auth/handler.py", offset=140, limit=20)  # 500 tokens

# 3. ìŠ¤ë‹ˆí« ì¶”ì¶œ
snippet = optimizer.extract_relevant_snippets(
    code,
    keywords=["authentication", "verify", "error"],
    context_lines=3
)  # 300 tokens

# 4. ìºì‹œ í™•ì¸ í›„ AI ì¿¼ë¦¬
prompt = f"Fix bug in:\n{snippet}"
cached = optimizer.get_cached_response(prompt)
if not cached:
    result = ai.query(prompt)  # 600 tokens
    optimizer.cache_response(prompt, result)
else:
    result = cached  # 0 tokens

# ì´: 1,500 tokens (90% ì ˆì•½)
```

### ì˜ˆì‹œ 2: ë¦¬íŒ©í† ë§

**âŒ ê¸°ì¡´ ë°©ì‹ (ë¹„íš¨ìœ¨)**
```python
# ëª¨ë“  ê´€ë ¨ íŒŒì¼ ì½ê¸° (50,000 tokens)
for file in all_python_files:
    code = read_file(file)
    dependencies = ai.query(f"Find dependencies in {code}")
```

**âœ… ìµœì í™” ë°©ì‹ (5,000 tokens)**
```python
from execution.system.dependency_analyzer import DependencyAnalyzer

analyzer = DependencyAnalyzer()

# 1. ì˜ì¡´ì„± ê·¸ë˜í”„ ìºì‹œì—ì„œ ë¡œë“œ
graph = analyzer.load_cached_graph()
if not graph:
    # ì²˜ìŒë§Œ ìƒì„± (ì „ì²´ í”„ë¡œì íŠ¸ í•œë²ˆë§Œ ë¶„ì„)
    graph = analyzer.build_dependency_graph()
    analyzer.cache_graph(graph)

# 2. ë¦¬íŒ©í† ë§ ëŒ€ìƒ íŒŒì¼ ì˜í–¥ ë²”ìœ„ë§Œ ì¶”ì¶œ
affected = analyzer.find_affected_files("target_module.py")
# â†’ 3ê°œ íŒŒì¼ë§Œ ì˜í–¥ë°›ìŒ

# 3. ì˜í–¥ë°›ëŠ” íŒŒì¼ì˜ ìš”ì•½ë§Œ ì½ê¸°
for file in affected:
    summary = analyzer.get_file_summary(file)
    # AIì—ê²Œ ì „ì²´ ì½”ë“œ ëŒ€ì‹  ìš”ì•½ë§Œ ì „ë‹¬

# ì´: 5,000 tokens (90% ì ˆì•½)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### ë§¤ì£¼ ì›”ìš”ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. ì£¼ê°„ ë¦¬í¬íŠ¸
python execution/system/weekly_optimization_monitor.py

# í™•ì¸ ì‚¬í•­:
# - Cache Hit Rate: 40% ì´ìƒ ìœ ì§€?
# - Tokens Saved: ì§€ë‚œì£¼ ëŒ€ë¹„ ì¦ê°€?
# - Overall Grade: B+ ì´ìƒ?

# 2. ìê°€ ê°œì„  ì‹¤í–‰
python execution/system/self_annealing_optimizer.py run

# 3. ê¶Œì¥ì‚¬í•­ í™•ì¸ ë° ì ìš©
# ë¦¬í¬íŠ¸ì— ë‚˜ì˜¨ HIGH priority í•­ëª© ë¨¼ì € ì²˜ë¦¬

# 4. ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬
python execution/system/token_optimizer.py clear 72  # 72ì‹œê°„ ì´ìƒ
```

---

## ğŸ”§ ê¸°ì¡´ ì½”ë“œì— í†µí•©í•˜ê¸°

### AI Engine í†µí•© (ì´ë¯¸ ì™„ë£Œ)

[ai_engine.py](../libs/ai_engine.py)ì— TokenOptimizerê°€ ìë™ìœ¼ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤:

```python
# libs/ai_engine.py
from execution.system.token_optimizer import TokenOptimizer

class AIEngine:
    def __init__(self):
        self.optimizer = TokenOptimizer()

    def generate_thought(self, prompt):
        # ìë™ìœ¼ë¡œ ìºì‹œ í™•ì¸
        cached = self.optimizer.get_cached_response(prompt)
        if cached:
            return cached

        # AI í˜¸ì¶œ
        response = self._call_api(prompt)

        # ìë™ìœ¼ë¡œ ìºì‹±
        self.optimizer.cache_response(prompt, response)
        return response
```

**ì„¤ì • ë¶ˆí•„ìš” â†’ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥**

### ìƒˆ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì‹œ

```python
#!/usr/bin/env python3
"""
Your New Script
"""
from execution.system.token_optimizer import TokenOptimizer
from execution.system.dependency_analyzer import DependencyAnalyzer

def your_function():
    optimizer = TokenOptimizer()
    analyzer = DependencyAnalyzer()

    # 1. ì˜ì¡´ì„± í™•ì¸
    affected = analyzer.find_affected_files("target.py")

    # 2. ìš”ì•½ë§Œ ì½ê¸°
    for file in affected:
        summary = analyzer.get_file_summary(file)

        # 3. ìºì‹œ í™•ì¸
        cached = optimizer.get_cached_response(f"analyze:{file}")
        if not cached:
            # AI ì²˜ë¦¬
            result = ai.query(summary)
            optimizer.cache_response(f"analyze:{file}", result)
        else:
            result = cached
```

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ

### ëª©í‘œ KPI

| ì§€í‘œ | ëª©í‘œ | í˜„ì¬ ìƒíƒœ í™•ì¸ |
|-----|------|--------------|
| ìºì‹œ íˆíŠ¸ìœ¨ | 40%+ | `weekly_optimization_monitor.py` |
| í‰ê·  ìš”ì²­ë‹¹ í† í° | 2,000 ì´í•˜ | ë¦¬í¬íŠ¸ì˜ `tokens_saved` ì°¸ê³  |
| ì£¼ê°„ ì ˆì•½ í† í° | 50,000+ | ë¦¬í¬íŠ¸ ì°¸ê³  |
| Overall Grade | B+ ì´ìƒ | ë¦¬í¬íŠ¸ ì°¸ê³  |

### ë“±ê¸‰ ê¸°ì¤€

- **A+ (90+)**: Excellent - ìµœì í™” ì™„ë²½
- **A (80-89)**: Very Good - ì˜ ìœ ì§€ ì¤‘
- **B+ (70-79)**: Good - ê°œì„  ì—¬ì§€ ìˆìŒ
- **B (60-69)**: Fair - ìµœì í™” í•„ìš”
- **C (50-59)**: Needs Improvement - ì¦‰ì‹œ ê°œì„  í•„ìš”
- **D (<50)**: Poor - ì‹œìŠ¤í…œ ì ê²€ í•„ìš”

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ìºì‹œ íˆíŠ¸ìœ¨ì´ 30% ë¯¸ë§Œ

**ì›ì¸**: ì¿¼ë¦¬ íŒ¨í„´ì´ ë„ˆë¬´ ë‹¤ì–‘í•˜ê±°ë‚˜ í”„ë¡¬í”„íŠ¸ê°€ ë§¤ë²ˆ ë‹¬ë¼ì§

**í•´ê²°**:
```python
# í”„ë¡¬í”„íŠ¸ í‘œì¤€í™”
# âŒ BAD: ë§¤ë²ˆ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸
f"Find bugs in this code: {code} at {datetime.now()}"

# âœ… GOOD: í‘œì¤€í™”ëœ í”„ë¡¬í”„íŠ¸
f"Find bugs in:\n{code_snippet}"
```

### ë¬¸ì œ 2: í† í° ì ˆì•½ì´ ì ìŒ

**ì›ì¸**: ì—¬ì „íˆ í° íŒŒì¼ì„ ì „ì²´ ì½ê³  ìˆìŒ

**í•´ê²°**:
```bash
# ìê°€ ê°œì„  ì‹¤í–‰
python execution/system/self_annealing_optimizer.py run

# Large prompts í™•ì¸
python execution/system/self_annealing_optimizer.py analyze

# í° í”„ë¡¬í”„íŠ¸ ë°œê²¬ ì‹œ ìŠ¤ë‹ˆí« ì¶”ì¶œìœ¼ë¡œ ë³€ê²½
```

### ë¬¸ì œ 3: Overall Gradeê°€ C ì´í•˜

**ì›ì¸**: ìµœì í™” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

**í•´ê²°**:
1. `directives/token_optimization_protocol.md` ë‹¤ì‹œ ì½ê¸°
2. 4ê°€ì§€ í•µì‹¬ ê·œì¹™ ì¤€ìˆ˜ í™•ì¸
3. ì½”ë“œì— TokenOptimizer í†µí•©
4. 1ì£¼ì¼ í›„ ì¬ì¸¡ì •

---

## ğŸ“š ë” ì•Œì•„ë³´ê¸°

- **ìƒì„¸ ê°€ì´ë“œ**: [directives/token_optimization_protocol.md](../directives/token_optimization_protocol.md)
- **ì‹œìŠ¤í…œ ê·œì¹™**: [CLAUDE.md](../CLAUDE.md#token-optimization-critical)
- **ìŠ¤í¬ë¦½íŠ¸ ì†ŒìŠ¤**:
  - [token_optimizer.py](../execution/system/token_optimizer.py)
  - [dependency_analyzer.py](../execution/system/dependency_analyzer.py)
  - [weekly_optimization_monitor.py](../execution/system/weekly_optimization_monitor.py)
  - [self_annealing_optimizer.py](../execution/system/self_annealing_optimizer.py)

---

## ğŸ“ 5ë¶„ íŠœí† ë¦¬ì–¼

### Step 1: í˜„ì¬ ìƒíƒœ í™•ì¸
```bash
python execution/system/token_optimizer.py report
```

### Step 2: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
python execution/system/token_optimizer.py test
```

### Step 3: ì²« ì£¼ê°„ ë¦¬í¬íŠ¸
```bash
python execution/system/weekly_optimization_monitor.py
```

### Step 4: ì½”ë“œì— ì ìš©
```python
from execution.system.token_optimizer import TokenOptimizer
optimizer = TokenOptimizer()

# ê¸°ì¡´ ì½”ë“œ ì „:
# result = ai.query(large_prompt)

# ê¸°ì¡´ ì½”ë“œ í›„:
snippet = optimizer.extract_relevant_snippets(large_prompt, keywords=["target"])
cached = optimizer.get_cached_response(snippet)
result = cached if cached else ai.query(snippet)
```

### Step 5: 1ì£¼ í›„ ì¬í™•ì¸
```bash
python execution/system/weekly_optimization_monitor.py
# í† í° ì ˆì•½ëŸ‰ í™•ì¸!
```

---

**ì‹œì‘í•˜ì„¸ìš”!** 5ë¶„ë§Œ íˆ¬ìí•˜ë©´ ì•ìœ¼ë¡œ ìˆ˜ê°œì›”ê°„ 60-80% í† í°ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì§ˆë¬¸/ì´ìŠˆ**: ì´ í”„ë¡œì íŠ¸ëŠ” ìê°€ ê°œì„  ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë¬¸ì œ ë°œê²¬ ì‹œ:
1. `execution/system/self_annealing_optimizer.py run` ì‹¤í–‰
2. ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³  directive ì—…ë°ì´íŠ¸
3. ì‹œìŠ¤í…œì´ ë” ê°•í•´ì§‘ë‹ˆë‹¤

---

**ì‘ì„±**: 2026-02-15
**ë²„ì „**: 1.0
**ì‘ì„±ì**: 97LAYER System
